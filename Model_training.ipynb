{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d814ba",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>AI vs. Human: Interpretable Binary Classification with CNNs on the <br>Dalle-Recognition Dataset</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46875e72",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "This Notebook offers a detailed exploration into images binary classification using Convolutional Nerual Networks (CNN), aiming to differentiate between images created by AI (Class: Fake) and those created by humans (Class: Real) within the \"Dalle-Recognition\" dataset. Key activities encompass:\n",
    "<br>\n",
    "<ul>\n",
    "<li><strong>Data Transformation and Loading</strong>: Demonstrates essential image transformations for normalization, followed by efficient loading techniques, to prepare the dataset for binary classification.</li>\n",
    "    \n",
    "<li><strong>Image Denoising</strong>: Given the unique challenge posed by AI-generated images, where models like DALL-E begin with a noise pattern to generate artwork based on textual prompts, distinguishing these from human-created images necessitates advanced denoising methods. The presence of inherent noise patterns can mislead classifiers into incorrectly tagging human-made art as AI-generated. By applying wavelet-based denoising, the aim is to neutralize this noise, thereby reducing confusion and improving the classification accuracy</li>\n",
    "    \n",
    "<li><strong>Model Preparation and Experiments</strong>: The notebook conducts rigorous training sessions. It systematically experiments with 6 distinct architectural designs and various learning rates to find the optimal setup for accurately classifying images into AI-generated or human-created categories.</li>\n",
    "    \n",
    "<li><strong>Performance Evaluation</strong>: Employs precise metrics—precision, recall, F1 score, and accuracy—and generates extensive classification reports and confusion matrices. These tools collectively offer in-depth insights into the model's capability to differentiate between the two classes of images effectively.</li>\n",
    "    \n",
    "<li><strong>Visualization Techniques</strong>: Incorporates a range of visualization functions to plot metrics over epochs, showcase samples from each dataset category, and illustrate class distribution, facilitating an intuitive grasp of the dataset specifics and the model's learning trajectory.</li>\n",
    "    \n",
    "<li><strong>Interpretability Analysis</strong>: Applies the Integrated Gradients technique for a granular analysis of the model's decision-making process. It highlights specific image features that significantly influence the classification outcome, providing transparency and understanding of the model's predictive behavior.</li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dca798",
   "metadata": {},
   "source": [
    "## Generals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6590cdf0",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Packages import and system configurations. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4cc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from random import sample\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "import matplotlib.cm as cm\n",
    "from scipy.ndimage import zoom\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report\n",
    "import pywt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc69bf1",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Datasets paths. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f6881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(current_path, 'io', 'input', 'dataset')\n",
    "train_data_path = os.path.join(dataset_path, 'train')\n",
    "test_data_path = os.path.join(dataset_path, 'test')\n",
    "metrics_plot_path = os.path.join(current_path, 'io', 'output', 'plots')\n",
    "saved_models_path = os.path.join(current_path, 'io', 'output', 'models')\n",
    "results_path = os.path.join(current_path, 'io', 'output', 'results')\n",
    "experimental_evaluation_path = os.path.join(metrics_plot_path, 'experimental_evaluation')\n",
    "interpretability_results_path = os.path.join(current_path, 'io', 'output', 'interpretability')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf22ce",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Setting Random Seeds for Reproducibility. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc913a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # For CUDA devices\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f9d9c",
   "metadata": {},
   "source": [
    "## Data Transformation & Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81031c66",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function applies wavelet transform-based denoising to an image, utilizing the discrete wavelet transform (DWT) for noise estimation and reduction across multiple levels of detail. It aims to mitigate noise by adjusting wavelet coefficients based on a calculated threshold, which is derived from the image's noise characteristics, to improve image clarity and quality. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_denoise(image_data, wavelet='db1', level=1):\n",
    "    # Perform discrete wavelet transform (DWT)\n",
    "    coeffs = pywt.wavedec2(image_data, wavelet, level=level)\n",
    "    \n",
    "    # Estimate noise sigma using the Median Absolute Deviation (MAD) of the highest level detail coefficients\n",
    "    sigma = np.median(np.abs(coeffs[-1][-1])) / 0.6745\n",
    "    \n",
    "    # Check if sigma is zero or NaN and adjust it\n",
    "    if sigma == 0 or np.isnan(sigma):\n",
    "        sigma = 1e-10  # A small positive value to avoid division by zero or NaN operations\n",
    "    \n",
    "    threshold = sigma * np.sqrt(2 * np.log(image_data.size))\n",
    "    \n",
    "    # Apply threshold to the detail coefficients at each level\n",
    "    coeffs_thresholded = [coeffs[0]] + [tuple(pywt.threshold(i, threshold, mode='soft') for i in detail) for detail in coeffs[1:]]\n",
    "    \n",
    "    # Reconstruct the image from the thresholded coefficients\n",
    "    denoised_data = pywt.waverec2(coeffs_thresholded, wavelet)\n",
    "    \n",
    "    return denoised_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65156f18",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function extends the wavelet-based denoising approach to RGB images by applying the wavelet_denoise method separately to each color channel and then recombining them. This approach ensures that noise is effectively reduced in each channel of the color image, enhancing overall image quality while preserving color integrity. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_denoise_color(image, wavelet='db1', level=1):\n",
    "    denoised_channels = []\n",
    "    for channel in range(3):  # Process each channel: R, G, B\n",
    "        # Extract the current channel\n",
    "        channel_data = image[:, :, channel]\n",
    "        # Denoise the current channel using the previous wavelet_denoise function\n",
    "        denoised_channel = wavelet_denoise(channel_data, wavelet=wavelet, level=level)\n",
    "        denoised_channels.append(denoised_channel)\n",
    "        \n",
    "    # Stack the denoised channels back together\n",
    "    denoised_image = np.stack(denoised_channels, axis=2)\n",
    "    return denoised_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c891fb2",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Theis class is a custom PyTorch transformation that applies wavelet-based denoising to each channel of an RGB image. Designed for integration into PyTorch's data preprocessing pipelines, it converts images to numpy arrays for denoising and then back to PIL images, enabling noise reduction in RGB images before they are passed into neural network models.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f91d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveletDenoiseTransformRGB:\n",
    "    def __init__(self, wavelet='db1', level=1):\n",
    "        self.wavelet = wavelet\n",
    "        self.level = level\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        # Convert PIL image to numpy array\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # Apply denoising for RGB image\n",
    "        denoised_np = wavelet_denoise_color(img_np, wavelet=self.wavelet, level=self.level)\n",
    "        \n",
    "        # Convert numpy array back to PIL Image\n",
    "        img_denoised = Image.fromarray(denoised_np.astype('uint8'), 'RGB')\n",
    "        return img_denoised\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fff0d7",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function loads and preprocesses image datasets for training and testing. It checks for dataset existence, applies transformations like resizing and converting to tensor, and returns the datasets along with class names and labels.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef814766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(train_data_path, test_data_path, img_height, img_width, denoising=False):\n",
    "    \n",
    "    if not os.path.exists(train_data_path) or not os.path.exists(test_data_path):\n",
    "        print('Preprocessed dataset does not exist, please created on the Data_preprocessing notebook')\n",
    "        return False, False, False, False\n",
    "      \n",
    "    if denoising:\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.Resize((img_height, img_width)),\n",
    "            WaveletDenoiseTransformRGB(),\n",
    "            transforms.ToTensor(),])\n",
    "    else:\n",
    "        data_transform = transforms.Compose([\n",
    "            transforms.Resize((img_height, img_width)),\n",
    "            transforms.ToTensor(),])\n",
    "\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=train_data_path, transform=data_transform)\n",
    "    test_dataset = torchvision.datasets.ImageFolder(root=test_data_path, transform=data_transform)\n",
    "    \n",
    "    class_names = train_dataset.classes\n",
    "    class_to_label = train_dataset.class_to_idx\n",
    "    \n",
    "    return train_dataset, test_dataset, class_names, class_to_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d258e",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function creates data loaders for training, validation, and testing datasets. It optionally splits the training dataset into training and validation sets based on a specified ratio, facilitating model evaluation during training. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(train_dataset, test_dataset, batch_size, train_val_split_ratio, use_validation_set):\n",
    "    \n",
    "    if use_validation_set:\n",
    "        train_size = int(train_val_split_ratio * len(train_dataset))\n",
    "        val_size = len(train_dataset) - train_size\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    else:\n",
    "        val_loader = []\n",
    "        \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf2f79",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caad22b3",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function displays a set of random images from a given dataset, alongside their class labels, in a grid format.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c77a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_images(dataset, class_names, dataset_name, num_images=6):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(5, 5))\n",
    "    fig.suptitle(f\"DALL-E Recognition Dataset\", fontsize=14)\n",
    "\n",
    "    for ax in axes.ravel():\n",
    "        # Choose a random index\n",
    "        idx = np.random.randint(len(dataset))\n",
    "\n",
    "        # Get the image and label at the random index\n",
    "        image, label = dataset[idx]\n",
    "\n",
    "        # Plot the image\n",
    "        ax.imshow(np.transpose(image.numpy(), (1, 2, 0)))\n",
    "        ax.set_title(f\"Class: {class_names[label]}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(metrics_plot_path + '/Random_Images' + '.pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7491b83d",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function assesses and visualizes the balance of image classes within train and test datasets. It calculates the number of images per class for both datasets, plots these counts in a comparative bar chart, and saves the chart as a PDF. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_balance(train_dataset, test_dataset, class_names):\n",
    "    # Calculate the number of images per class in train dataset\n",
    "    train_class_counts = {class_name: 0 for class_name in class_names}\n",
    "    for _, label in train_dataset:\n",
    "        class_name = class_names[label]\n",
    "        train_class_counts[class_name] += 1\n",
    "\n",
    "    # Calculate the number of images per class in test dataset\n",
    "    test_class_counts = {class_name: 0 for class_name in class_names}\n",
    "    for _, label in test_dataset:\n",
    "        class_name = class_names[label]\n",
    "        test_class_counts[class_name] += 1\n",
    "\n",
    "    # Plotting\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    class_labels = list(train_class_counts.keys())\n",
    "    x = range(len(class_labels))\n",
    "\n",
    "    ax.bar(x, train_class_counts.values(), width=0.4, align='center', label='Train Set')\n",
    "    ax.bar(x, test_class_counts.values(), width=0.4, align='edge', label='Test Set')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_labels)\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Number of Images')\n",
    "    ax.set_title('Number of Images per Class in Train and Test Sets')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(metrics_plot_path + '/Class_Balance' + '.pdf')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d2b7d0",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function prints the shape of the first batch of images and labels from a specified data loader, providing a quick overview of batch size and image dimensions. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ea6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader_shapes(loader, loader_name):\n",
    "    print(f\"Shapes of batches in {loader_name}:\")\n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        print(f\"Batch {i+1}: {images.shape}, {labels.shape}\\n\")\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48884ad9",
   "metadata": {},
   "source": [
    "## CNN Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7636d6",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This class defines a flexible CNN model for image classification, allowing for variations in architecture based on the model name provided during initialization. It includes configurations for different numbers of convolutional layers, pooling layers, batch normalization, and dropout, enabling experimentation with model complexity and regularization techniques.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7402df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self, model_name, input_channels):\n",
    "        super(CNN_Model, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Define common layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer remains the same for all models\n",
    "        \n",
    "        # Additional layers for models with batch normalization and more pooling\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # Different first fully connected layers for different models\n",
    "        self.fc1_model1 = nn.Linear(32 * 64 * 64, 128)\n",
    "        self.fc1_model2 = nn.Linear(64 * 32 * 32, 128)\n",
    "        self.fc1_model3 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc1_model4 = nn.Linear(128 * 8 * 8, 128)\n",
    "        self.fc1_model5 = nn.Linear(128 * 8 * 8, 128)\n",
    "        self.fc1_model6 = nn.Linear(128 * 4 * 4, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model_name == '2Conv1Pool':\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = x.view(-1, 64 * 64 * 32)\n",
    "            x = F.relu(self.fc1_model1(x))\n",
    "            x = torch.sigmoid(self.fc2(x))\n",
    "        \n",
    "        if self.model_name == '3Conv2Pool':\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = self.pool(F.relu(self.conv3(x)))\n",
    "            x = x.view(-1, 64 * 32 * 32)\n",
    "            x = F.relu(self.fc1_model2(x))\n",
    "            x = torch.sigmoid(self.fc2(x))\n",
    "            \n",
    "        elif self.model_name == '3Conv3Pool':\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = self.pool(F.relu(self.conv3(x)))\n",
    "            x = x.view(-1, 64 * 16 * 16)\n",
    "            x = F.relu(self.fc1_model3(x))\n",
    "            x = torch.sigmoid(self.fc2(x))      \n",
    "        \n",
    "        elif self.model_name == '4Conv4Pool':\n",
    "            x = self.pool(F.relu(self.conv1(x)))\n",
    "            x = self.pool(F.relu(self.conv2(x))) \n",
    "            x = self.pool(F.relu(self.conv3(x)))  \n",
    "            x = self.pool(F.relu(self.conv4(x)))\n",
    "            x = x.view(-1, 128 * 8 * 8)\n",
    "            x = F.relu(self.fc1_model4(x))\n",
    "            x = torch.sigmoid(self.fc2(x))\n",
    "        \n",
    "        elif self.model_name == '4Conv4Pool_BatchNorm':\n",
    "            x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = self.pool(F.relu(self.conv3(x)))\n",
    "            x = self.pool(F.relu(self.conv4(x)))\n",
    "            x = x.view(-1, 128 * 8 * 8)\n",
    "            x = F.relu(self.fc1_model5(x))\n",
    "            x = torch.sigmoid(self.fc2(x))\n",
    "        \n",
    "        elif self.model_name == '4Conv4Pool_BatchNorm_Dropout':\n",
    "            x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "            x = self.pool(F.relu(self.conv4(x)))\n",
    "            x = self.pool(x)\n",
    "            x = self.dropout(x)\n",
    "            x = x.view(-1, 128 * 4 * 4)\n",
    "            x = F.relu(self.fc1_model6(x))\n",
    "            x = torch.sigmoid(self.fc2(x))\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ff2a91",
   "metadata": {},
   "source": [
    "## Models Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26feb94",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function evaluates a model's performance on a validation set by calculating key metrics: accuracy, precision, recall, and F1 score. It processes the validation data in batches, generating predictions, and then converts these into binary outcomes based on a specified threshold.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, device, val_loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    predicted_labels = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            predicted_labels.extend(outputs.cpu().detach().numpy().flatten())\n",
    "            true_labels.extend(labels.cpu().detach().numpy())\n",
    "    \n",
    "    # Convert predicted probabilities to binary predictions based on threshold\n",
    "    predicted_labels_binary = (np.array(predicted_labels) >= threshold).astype(np.float32)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels_binary)\n",
    "    precision = precision_score(true_labels, predicted_labels_binary)\n",
    "    recall = recall_score(true_labels, predicted_labels_binary)\n",
    "    f1 = f1_score(true_labels, predicted_labels_binary)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb404d",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "This function orchestrates the training process for a given model over a specified number of epochs, optionally performing validation. Here's a breakdown of its steps and objectives:\n",
    "<br>\n",
    "<ol>\n",
    "<li>Initialization: Sets up the optimizer (Adam) with a given learning rate, loss function (Binary Cross Entropy Loss), and moves the model to the designated computing device.</li>\n",
    "<li>Training Loop</li>\n",
    "<li>Validation (Optional): If validation mode is enabled, evaluates the model on the validation set after each training epoch using the calculate_metrics function, then prints and records these metrics.</li>\n",
    "<li>Metrics Tracking: Accumulates training and, if enabled, validation metrics over all epochs to monitor performance trends.</li>\n",
    "<li>Output: Returns the trained model and a dictionary of metrics for further analysis.</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_loader, val_loader, learning_rate, num_epochs, validation_mode):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    model.to(device)\n",
    "\n",
    "    train_loss_ls, train_accuracy_ls, train_precision_ls, train_recall_ls, train_f1_ls = [], [], [], [], []\n",
    "    eval_accuracy_ls, eval_precision_ls, eval_recall_ls, eval_f1_ls = [], [], [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in tqdm(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels.view(-1, 1).float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        train_accuracy, train_precision, train_recall, train_f1 = calculate_metrics(model, device, train_loader)\n",
    "        print(\"Epoch {}, Train || Cross Entropy Loss: {:.3f}\".format(epoch+1, epoch_loss))\n",
    "        print(\"Epoch {}, Train || Accuracy: {:.3f}, Precision: {:.3f}, Recall: {:.3f}, F1-Score: {:.3f}\".format(epoch+1, train_accuracy, train_precision, train_recall, train_f1))\n",
    "        train_loss_ls.append(epoch_loss)\n",
    "        train_accuracy_ls.append(train_accuracy)\n",
    "        train_precision_ls.append(train_precision)\n",
    "        train_recall_ls.append(train_recall)\n",
    "        train_f1_ls.append(train_f1)\n",
    "                                                                                                                \n",
    "        if validation_mode:\n",
    "            eval_accuracy, eval_precision, eval_recall, eval_f1 = calculate_metrics(model, device, val_loader)\n",
    "            print(\"Epoch {}, Evaluation || Accuracy: {:.3f}, Precision: {:.3f}, Recall: {:.3f}, F1-Score: {:.3f}\".format(epoch+1, eval_accuracy, eval_precision, eval_recall, eval_f1))        \n",
    "            eval_accuracy_ls.append(eval_accuracy)\n",
    "            eval_precision_ls.append(eval_precision)\n",
    "            eval_recall_ls.append(eval_recall)\n",
    "            eval_f1_ls.append(eval_f1)\n",
    "        \n",
    "        metrics = {'train_loss': train_loss_ls,'train_accuracy': train_accuracy_ls, 'train_precision': train_precision_ls,\n",
    "                   'train_recall': train_recall_ls, 'train_f1': train_f1_ls, 'eval_accuracy': eval_accuracy_ls,\n",
    "                   'eval_precision': eval_precision_ls, 'eval_recall': eval_recall_ls, 'eval_f1': eval_f1_ls}\n",
    "        \n",
    "    return model, metrics \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0d0e16",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function generates a classification report for a given model on a the test dataset. It evaluates the model's performance by comparing its predictions against the true labels, providing detailed insights into metrics like precision, recall, and F1-score for each class.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067fc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_report(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            pred_labels.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    report = classification_report(true_labels, pred_labels, output_dict=False)\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f146e7",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function initializes the weights of convolutional and linear layers in a neural network using Xavier uniform initialization for weights and setting biases to zero, ensuring optimal starting points for training.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b679f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f36c2",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function saves a dictionary to a JSON file at the specified path and filename.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef53583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(path, filename, data):\n",
    "    with open(path + '/' + filename + '.json', 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b65fa",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function loads a dictionary from a JSON file using the given path and filename.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d508c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(path, filename):\n",
    "    with open(path + '/' + filename + '.json', 'r') as f:\n",
    "        data_loaded = json.load(f)\n",
    "    return data_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81a368",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function saves a model's state dictionary to a specified file, facilitating model persistence and later retrieval.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36480e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "    }, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c205921",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function loads a model's state dictionary from a specified file, updating the model for further use or evaluation.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60227156",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf64c0",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "This function calculates the Integrated Gradients for a single image from the test loader to interpret a model's predictions, specifically highlighting the importance of input features (pixels) in the prediction of a class (real or fake). Here are the core steps:\n",
    "<br>\n",
    "<ol>\n",
    "<li>Fetch a Batch: Retrieves a batch of images and labels from the test loader and focuses on the first image and label.</li>\n",
    "<li>Prepare the Input: Puts the selected image in a tensor suitable for the model (adds a batch dimension).</li>\n",
    "<li>Model Evaluation: Switches the model to evaluation mode and computes the output for the input image</li>\n",
    "<li>Predicted Probabilities: Applies the sigmoid function to the model's output to get the predicted probabilities for being real and fake.</li>\n",
    "<li>Class Name Resolution: Identifies the class name of the target label using the class_to_label mapping.</li>\n",
    "<li>Integrated Gradients Calculation:Generates attributions using Integrated Gradients, indicating the importance of each pixel for the model's prediction.</li>\n",
    "<li>Result Compilation: Constructs a string summarizing the model's prediction and prepares the attributions for visualization.</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0537af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_integrated_gradients(model, test_loader, class_to_label, index_image):\n",
    "    # Fetch a batch of images and labels\n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "\n",
    "    # Select the first image in the batch\n",
    "    input_tensor = images[index_image].unsqueeze(0) \n",
    "    target_label = labels[index_image].item() \n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():  \n",
    "        output = model(input_tensor)  \n",
    "\n",
    "    predicted_prob = torch.sigmoid(output) \n",
    "    # Calculate the probability of the opposite class\n",
    "    predicted_prob_fake = 1 - predicted_prob.item()\n",
    "    predicted_prob_real = predicted_prob.item()\n",
    "    # Find the class names\n",
    "    target_class_name = [name for name, label in class_to_label.items() if label == target_label][0]\n",
    "    result = f\"Target class:{target_class_name} \\n The model predicts REAL:{predicted_prob_real:.2f}% & FAKE:{predicted_prob_fake:.2f}%\"\n",
    "\n",
    "    # Initialize Integrated Gradients\n",
    "    integrated_gradients = IntegratedGradients(model)\n",
    "    # Generate attributions\n",
    "    attributions = integrated_gradients.attribute(input_tensor, target=0, n_steps=50)\n",
    "    attributions_np = attributions.detach().cpu().numpy()[0]\n",
    "    attributions_sum = attributions_np.sum(axis=0)\n",
    "    attributions_sum = attributions_sum / np.max(np.abs(attributions_sum))\n",
    "    return result, attributions_sum, input_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5037e6e",
   "metadata": {},
   "source": [
    "## Results Plotting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17aa2e3",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function smooths a series of data points using exponential moving average, reducing noise for clearer trend visualization.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edddae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.6):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1517b066",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function plots a smoothed version of a given metric over epochs, saves the plot as a PDF, and displays it, aiding in performance analysis.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_metric(metric, metric_label, metrics_plot_path, set_name, title):\n",
    "    smooth_metric = smooth_curve(metric)\n",
    "    plt.plot(range(1, len(smooth_metric) + 1), smooth_metric, label=set_name)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_label)\n",
    "    plt.legend()\n",
    "    plt.savefig(metrics_plot_path + '/' + set_name + '_' + metric_label + '.pdf')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6dedd",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function plots and saves a comprehensive view of training and evaluation metrics (accuracy, precision, recall, F1-score) over epochs, using subplots for clear comparison and analysis of model performance trends.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics_train_val(metrics, plot_name, metrics_plot_path, evaluation_mode, title):\n",
    "    smooth_accuracy_train = smooth_curve(metrics['train_accuracy'])\n",
    "    smooth_accuracy_eval = smooth_curve(metrics['eval_accuracy'])\n",
    "    smooth_precision_train = smooth_curve(metrics['train_precision'])\n",
    "    smooth_precision_eval = smooth_curve(metrics['eval_precision'])\n",
    "    smooth_recall_train = smooth_curve(metrics['train_recall'])\n",
    "    smooth_recall_eval = smooth_curve(metrics['eval_recall'])\n",
    "    smooth_f1_train = smooth_curve(metrics['train_f1'])\n",
    "    smooth_f1_eval = smooth_curve(metrics['eval_f1'])\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2,figsize=(12, 8))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    # Plot the Accuracy metric on the top-left subplot\n",
    "    axs[0, 0].plot(range(1, len(smooth_accuracy_train) + 1), smooth_accuracy_train, label='Train')\n",
    "    axs[0, 0].plot(range(1, len(smooth_accuracy_eval) + 1), smooth_accuracy_eval, label=evaluation_mode)\n",
    "    axs[0, 0].set_xlabel('Epochs')\n",
    "    axs[0, 0].set_ylabel('Accuracy')\n",
    "    axs[0, 0].set_title('Accuracy')\n",
    "    axs[0, 0].legend()\n",
    "    # Plot the Precision metric on the top-right subplot\n",
    "    axs[0, 1].plot(range(1, len(smooth_precision_train) + 1), smooth_precision_train, label='Train')\n",
    "    axs[0, 1].plot(range(1, len(smooth_precision_eval) + 1), smooth_precision_eval, label=evaluation_mode)\n",
    "    axs[0, 1].set_xlabel('Epochs')\n",
    "    axs[0, 1].set_ylabel('Precision')\n",
    "    axs[0, 1].set_title('Precision')\n",
    "    axs[0, 1].legend()\n",
    "    # Plot the Recall metric on the bottom-left subplot\n",
    "    axs[1, 0].plot(range(1, len(smooth_recall_train) + 1), smooth_recall_train, label='Train')\n",
    "    axs[1, 0].plot(range(1, len(smooth_recall_eval) + 1), smooth_recall_eval, label=evaluation_mode)\n",
    "    axs[1, 0].set_xlabel('Epochs')\n",
    "    axs[1, 0].set_ylabel('Recall')\n",
    "    axs[1, 0].set_title('Recall')\n",
    "    axs[1, 0].legend()\n",
    "    # Plot the F1-Score metric on the bottom-right subplot\n",
    "    axs[1, 1].plot(range(1, len(smooth_f1_train) + 1), smooth_f1_train, label = 'Train')\n",
    "    axs[1, 1].plot(range(1, len(smooth_f1_eval) + 1), smooth_f1_eval, label = evaluation_mode)\n",
    "    axs[1, 1].set_xlabel('Epochs')\n",
    "    axs[1, 1].set_ylabel('F1-Score')\n",
    "    axs[1, 1].set_title('F1-Score')\n",
    "    axs[1, 1].legend()\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "    # Show the plot\n",
    "    plt.savefig(metrics_plot_path + '/' + plot_name + '_All_Metrics' + '.pdf')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9935a",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "This function visualizes an original image alongside its attribution heatmap to interpret the model's focus, and saves the combined plot for review.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d7551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interpretability(interpretability_results_path, result, attributions_sum, input_tensor, index_image):\n",
    "\n",
    "    input_image_np = input_tensor.detach().cpu().numpy()[0] \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4)) \n",
    "    \n",
    "    # Visualize the image and the attributions\n",
    "    ax[0].imshow(np.transpose(input_image_np, (1, 2, 0)))\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Original Image', fontsize=9)\n",
    "    ax[1].imshow(attributions_sum, cmap='jet')\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('Attribution Heatmap', fontsize=9)\n",
    "    \n",
    "\n",
    "    plt.tight_layout(pad=5.0)\n",
    "    plt.suptitle(result, fontsize=11, verticalalignment='top')\n",
    "    plt.savefig(interpretability_results_path + '/' + 'interpretability' + str(index_image) + '.pdf', bbox_inches='tight')  # This option reduces the padding around the saved figure\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d072fce",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "This function generates an overlayed visualization by combining the original image with a heatmap based on Integrated Gradients attributions, highlighting regions influencing the model's prediction. Here are the core steps:\n",
    "<br>\n",
    "<ol>\n",
    "<li>Normalize and prepare the input image and attributions for visualization, converting tensor data to NumPy arrays and applying normalization to both.</li>\n",
    "<li>Threshold attributions to identify key regions influencing the model's predictions, creating a binary mask for significant areas.</li>\n",
    "<li>Generate a heatmap from the normalized attributions using a colormap, emphasizing areas of interest.</li>\n",
    "<li>Resize and mask the heatmap to match the original image's dimensions and apply the binary mask, focusing on significant regions.</li>\n",
    "<li>Overlay the heatmap onto the original image with adjusted transparency, blending significant attribution areas with the original visual content.</li>\n",
    "<li>Display and save the combined overlay image, offering a visual interpretation of the model's decision-making areas.</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8dcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interpretability_overlayed_image(interpretability_results_path, result, attributions_sum, input_tensor, index_image, threshold=0.8):\n",
    "    # Convert the input tensor to numpy for visualization\n",
    "    input_image_np = input_tensor.detach().cpu().numpy()[0]  # Assuming the input is (1, C, H, W)\n",
    "    input_image_np = np.transpose(input_image_np, (1, 2, 0))  # Convert to H, W, C for visualization\n",
    "    \n",
    "    # Normalize the input image\n",
    "    input_image_np -= input_image_np.min()\n",
    "    input_image_np /= input_image_np.max()\n",
    "    \n",
    "    # Normalize the attribution sum to 0-1 for the heatmap\n",
    "    attributions_norm = (attributions_sum - attributions_sum.min()) / (attributions_sum.max() - attributions_sum.min())\n",
    "    \n",
    "    # Apply threshold to attributions\n",
    "    mask = attributions_norm > threshold\n",
    "    # Expand mask for RGB dimensions\n",
    "    mask = np.stack([mask, mask, mask], axis=2)\n",
    "    \n",
    "    # Create heatmap\n",
    "    heatmap = cm.jet(attributions_norm)[:, :, :3]  # Use the jet colormap, take the first 3 channels for RGB\n",
    "\n",
    "    # Resize heatmap to match the input image dimensions if necessary\n",
    "    if heatmap.shape != input_image_np.shape:\n",
    "        zoom_factors = (input_image_np.shape[0] / heatmap.shape[0], input_image_np.shape[1] / heatmap.shape[1], 1)\n",
    "        heatmap = zoom(heatmap, zoom_factors, order=1)\n",
    "    \n",
    "    # Apply the mask to the heatmap\n",
    "    heatmap_masked = np.where(mask, heatmap, np.zeros_like(heatmap))\n",
    "    \n",
    "    # Overlay the masked heatmap on the original image\n",
    "    overlayed_image = (1 * input_image_np + 0.4 * heatmap_masked).clip(0, 1)  # Adjust alpha to control the transparency\n",
    "    \n",
    "    # Display the overlayed image\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(overlayed_image)\n",
    "    plt.axis('off')\n",
    "    plt.title(result, fontsize=10)\n",
    "    plt.savefig(interpretability_results_path + '/' +'interpretability_overlayed_heatmap' + str(index_image) + '.pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925233e",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>Execution</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0547f69",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Global Variables\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f950a804",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 128\n",
    "img_width = 128\n",
    "input_channels = 3\n",
    "batch_size = 64\n",
    "train_val_split_ratio = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ee3e7",
   "metadata": {},
   "source": [
    "## Execution: Data Loading & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, class_names, class_to_label = load_datasets(train_data_path, test_data_path, img_height, img_width, denoising=False)\n",
    "\n",
    "#plot_class_balance(train_dataset, test_dataset, class_names)\n",
    "plot_random_images(train_dataset, class_names, \"Train Set\")\n",
    "print(\"Class to label mapping:\", class_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a1adf",
   "metadata": {},
   "source": [
    "## Execution: Tunning Using Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad847a0",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Local Variables\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9f49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoising = True\n",
    "use_validation_set = True\n",
    "num_epochs = 20\n",
    "learning_rate = 0.005\n",
    "set_name = 'Validation'\n",
    "\n",
    "networks = ['2Conv1Pool','3Conv2Pool', '3Conv3Pool', '4Conv4Pool',\n",
    "            '4Conv4Pool_BatchNorm','4Conv4Pool_BatchNorm_Dropout']\n",
    "\n",
    "final_arcitecture = '4Conv4Pool'\n",
    "learning_rate_ls = [0.0001, 0.001, 0.005]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa08dab",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Data Loading\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, class_names, class_to_label = load_datasets(train_data_path, test_data_path, img_height, img_width, denoising)\n",
    "train_loader, val_loader, test_loader = get_data_loaders(train_dataset, test_dataset, batch_size, train_val_split_ratio, use_validation_set)\n",
    "\n",
    "get_loader_shapes(train_loader, \"Train Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc7a64",
   "metadata": {},
   "source": [
    "### Experiments with Different CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126312b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for network in networks:\n",
    "    print(f\"Training of {network}\")\n",
    "    model = CNN_Model(network, input_channels)\n",
    "    model.apply(init_weights)\n",
    "    model, metrics_val = train_model(model, device, train_loader, val_loader, learning_rate, num_epochs, validation_mode=True)\n",
    "    results_name = network + '_' + set_name\n",
    "    save_dict(results_path, results_name, metrics_val)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a7277",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Analize and Plot Different CNN Results\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44655919",
   "metadata": {},
   "outputs": [],
   "source": [
    "for network in networks:\n",
    "    title = f\"Model: {network}\"\n",
    "    results_name = network + '_' + set_name\n",
    "    metrics_val = load_dict(results_path, results_name)\n",
    "    plot_all_metrics_train_val(metrics_val, results_name, metrics_plot_path, set_name, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280b0cf",
   "metadata": {},
   "source": [
    "### Experiments with Different Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e373ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate_value in learning_rate_ls:\n",
    "    print(f\"Training of {final_arcitecture} with Learning Rate: {learning_rate_value}\")\n",
    "    model = CNN_Model(final_arcitecture, input_channels)\n",
    "    model.apply(init_weights)\n",
    "    model, metrics_val = train_model(model, device, train_loader, val_loader, learning_rate_value, num_epochs, validation_mode=True)\n",
    "    results_name = final_arcitecture + '_lr_' + str(learning_rate_value) + '_' + set_name\n",
    "    save_dict(results_path, results_name, metrics_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1385099a",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Analize and Plot Learning Rate Influence\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate_value in learning_rate_ls:\n",
    "    title = f\"Model: {final_arcitecture} with LR: {learning_rate_value}\"\n",
    "    results_name = final_arcitecture + '_lr_' + str(learning_rate_value) + '_' + set_name\n",
    "    metrics_val = load_dict(results_path, results_name)\n",
    "    plot_all_metrics_train_val(metrics_val, results_name, metrics_plot_path, set_name, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98602bd",
   "metadata": {},
   "source": [
    "## Execution: Final Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c228b8",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Local Variables\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b8340",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoising = True\n",
    "set_name = 'Test'\n",
    "final_arcitecture = '4Conv4Pool'\n",
    "results_name = final_arcitecture + '_' + set_name\n",
    "num_epochs = 8\n",
    "learning_rate = 0.001\n",
    "use_validation_set = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41889c72",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Data Loading\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e163603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, class_names, class_to_label = load_datasets(train_data_path, test_data_path, img_height, img_width, denoising)\n",
    "train_loader, val_loader, test_loader = get_data_loaders(train_dataset, test_dataset, batch_size, train_val_split_ratio, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450b462",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Models Trainning\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b8da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model(final_arcitecture, input_channels)\n",
    "model.apply(init_weights)\n",
    "model, metrics_test = train_model(model, device, train_loader, test_loader, learning_rate, num_epochs, validation_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a05cb8",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Save Model and Results\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f785a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, saved_models_path + '/' + final_arcitecture + '.pth')\n",
    "save_dict(results_path, results_name, metrics_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4d82f",
   "metadata": {},
   "source": [
    "## Execution: Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8b114",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Local Variables\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd54388",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoising = True\n",
    "set_name = 'Test'\n",
    "final_arcitecture = '4Conv4Pool'\n",
    "use_validation_set = False\n",
    "results_name = final_arcitecture + '_' + set_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e14e73",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Data Loading \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f38b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, class_names, class_to_label = load_datasets(train_data_path, test_data_path, img_height, img_width, denoising)\n",
    "train_loader, val_loader, test_loader = get_data_loaders(train_dataset, test_dataset, batch_size, train_val_split_ratio, use_validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448227a",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Metrics Calculation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3160251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Model(final_arcitecture, input_channels)\n",
    "model = load_model(model, saved_models_path + '/' + final_arcitecture + '.pth')\n",
    "test_accuracy, test_precision, test_recall, test_f1 = calculate_metrics(model, device, test_loader)\n",
    "print(\"Evaluation on Test || Accuracy: {:.3f}, Precision: {:.3f}, Recall: {:.3f}, F1-Score: {:.3f}\".format(test_accuracy, test_precision, test_recall, test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac54b863",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Plot Test Metrics\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_test = load_dict(results_path, results_name)\n",
    "title = f\"Model: {final_arcitecture}\"\n",
    "plot_all_metrics_train_val(metrics_test, results_name, metrics_plot_path, set_name, title)\n",
    "plot_single_metric(metrics_test['eval_f1'], 'F1-Score', metrics_plot_path, set_name, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a5393",
   "metadata": {},
   "source": [
    "## Execution: Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae3754",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Local Variables\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoising = True\n",
    "final_arcitecture = '4Conv4Pool'\n",
    "use_validation_set = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5645dfe",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Data Loading \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, class_names, class_to_label = load_datasets(train_data_path, test_data_path, img_height, img_width, denoising)\n",
    "train_loader, val_loader, test_loader = get_data_loaders(train_dataset, test_dataset, batch_size, train_val_split_ratio, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58017a1f",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Calculate Integrated Gradients\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296be875",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_image = 30\n",
    "model = CNN_Model(final_arcitecture, input_channels)\n",
    "model = load_model(model, saved_models_path + '/' + final_arcitecture + '.pth')\n",
    "result, attributions_sum, input_tensor = calculate_integrated_gradients(model, test_loader, class_to_label, index_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffd814",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Plot Interpretability\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc676280",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interpretability(interpretability_results_path, result, attributions_sum, input_tensor, index_image)\n",
    "plot_interpretability_overlayed_image(interpretability_results_path, result, attributions_sum, input_tensor, index_image, threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9f4d13",
   "metadata": {},
   "source": [
    "## Experimental Evaluaiton Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0877f2e3",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Model Architecture\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1864645",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = load_dict(results_path, '2Conv1Pool_Validation')\n",
    "m2 = load_dict(results_path, '3Conv2Pool_Validation')\n",
    "m3 = load_dict(results_path, '3Conv3Pool_Validation')\n",
    "m4 = load_dict(results_path, '4Conv4Pool_Validation')\n",
    "m5 = load_dict(results_path, '4Conv4Pool_BatchNorm_Validation')\n",
    "m6 = load_dict(results_path, '4Conv4Pool_BatchNorm_Dropout_Validation')\n",
    "\n",
    "smooth_mae_history1 = smooth_curve(m1['eval_f1'][0:])\n",
    "smooth_mae_history2 = smooth_curve(m2['eval_f1'][0:])\n",
    "smooth_mae_history3 = smooth_curve(m3['eval_f1'][0:])\n",
    "smooth_mae_history4 = smooth_curve(m4['eval_f1'][0:])\n",
    "smooth_mae_history5 = smooth_curve(m5['eval_f1'][0:])\n",
    "smooth_mae_history6 = smooth_curve(m6['eval_f1'][0:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history1) + 1), smooth_mae_history1,label='2Conv1Pool')\n",
    "plt.plot(range(1, len(smooth_mae_history2) + 1), smooth_mae_history2,label='3Conv2Pool')\n",
    "plt.plot(range(1, len(smooth_mae_history3) + 1), smooth_mae_history3,label='3Conv3Pool')\n",
    "plt.plot(range(1, len(smooth_mae_history4) + 1), smooth_mae_history4,label='4Conv4Pool')\n",
    "plt.plot(range(1, len(smooth_mae_history5) + 1), smooth_mae_history5,label='4Conv4PoolBN')\n",
    "plt.plot(range(1, len(smooth_mae_history6) + 1), smooth_mae_history6,label='4Conv4PoolBNDR')\n",
    "\n",
    "plt.title('Model Architecture')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.legend()\n",
    "#plt.savefig(experimental_evaluation_path + \"/Model_Architecture.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949c682",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Learning Rate Infuence\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34026e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = load_dict(results_path, '4Conv4Pool_lr_0.0001_Validation')\n",
    "m2 = load_dict(results_path, '4Conv4Pool_lr_0.001_Validation')\n",
    "m3 = load_dict(results_path, '4Conv4Pool_lr_0.005_Validation')\n",
    "\n",
    "smooth_mae_history1 = smooth_curve(m1['eval_f1'][0:])\n",
    "smooth_mae_history2 = smooth_curve(m2['eval_f1'][0:])\n",
    "smooth_mae_history3 = smooth_curve(m3['eval_f1'][0:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history1) + 1), smooth_mae_history1,label='LR: 0.0001')\n",
    "plt.plot(range(1, len(smooth_mae_history2) + 1), smooth_mae_history2,label='LR: 0.001')\n",
    "plt.plot(range(1, len(smooth_mae_history3) + 1), smooth_mae_history3,label='LR: 0.005')\n",
    "\n",
    "\n",
    "plt.title('Learning Rate Infuence')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.legend()\n",
    "plt.savefig(experimental_evaluation_path + \"/Learning_Rate_Infuence.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10adf63c",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Validation vs Training: F1-Score\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f27e768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1 = load_dict(results_path, '4Conv4Pool_Validation')\n",
    "\n",
    "\n",
    "smooth_mae_history1 = smooth_curve(m1['eval_f1'][0:])\n",
    "smooth_mae_history2 = smooth_curve(m1['train_f1'][0:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history1) + 1), smooth_mae_history1,label='Validation')\n",
    "plt.plot(range(1, len(smooth_mae_history2) + 1), smooth_mae_history2,label='Training')\n",
    "\n",
    "plt.title('Validation vs Training: F1-Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.legend()\n",
    "plt.savefig(experimental_evaluation_path + \"/Validation_vs_Training.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26f458",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Wavelet Denoising Infuence\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = load_dict(results_path, '4Conv4Pool_Test_noise')\n",
    "m2 = load_dict(results_path, '4Conv4Pool_Test')\n",
    "\n",
    "smooth_mae_history1 = smooth_curve(m1['eval_f1'][0:])\n",
    "smooth_mae_history2 = smooth_curve(m2['eval_f1'][0:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history1) + 1), smooth_mae_history1,label='Raw Images')\n",
    "plt.plot(range(1, len(smooth_mae_history2) + 1), smooth_mae_history2,label='Wavelet Denoising')\n",
    "\n",
    "plt.title('Wavelet Denoising Infuence')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.legend()\n",
    "plt.savefig(experimental_evaluation_path + \"/Wavelet_Denoising.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e618020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full_ml",
   "language": "python",
   "name": "full_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
