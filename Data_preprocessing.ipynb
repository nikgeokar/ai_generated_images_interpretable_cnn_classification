{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be8d96ac",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>DALLE-Recongnition Dataset Preparation</center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504315ef",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "This notebook meticulously prepares an image dataset (DALLE Recongnition Dataset) for model training, ensuring the data is in the optimal state. Here's a high-level summary of the tasks applied within the notebook:\n",
    "<br><br>\n",
    "<ul>\n",
    "<li><strong>Random Seed Setting</strong>: Initializes random number generators with a fixed seed to ensure reproducibility across experiments.</li>\n",
    "<li><strong>Data Cleaning</strong>: Removes non-image files, excessively large images, and images with transparency, creating a clean and uniform dataset.</li>\n",
    "<li><strong>Dataset Balancing</strong>: Adjusts the number of images across different classes to ensure balance, crucial for unbiased model training.</li>\n",
    "<li><strong>Train-Test Split</strong>: Distributes images into training and testing sets based on a predefined ratio, preparing the dataset for both training and validation phases.</li>\n",
    "<li><strong>Directory Management</strong>: Performs checks on directories to ensure they are appropriately populated or cleaned up after preprocessing, maintaining an organized file structure for easy access and processing.</li>\n",
    "</ul>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb334ad",
   "metadata": {},
   "source": [
    "## Generals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95251796",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Packages import and system configurations. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from random import sample\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "import matplotlib.cm as cm\n",
    "from scipy.ndimage import zoom\n",
    "import torch.nn.functional as F\n",
    "\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0b5d5",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Datasets paths. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9da1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(current_path, 'io', 'input', 'dataset')\n",
    "train_data_path = os.path.join(dataset_path, 'train')\n",
    "test_data_path = os.path.join(dataset_path, 'test')\n",
    "\n",
    "inti_dataset_path = os.path.join(current_path, 'archive')\n",
    "init_real_class_path = os.path.join(inti_dataset_path, 'real')\n",
    "init_fake_class_path = os.path.join(inti_dataset_path, 'fakeV2', 'fake-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8113264d",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "Setting Random Seeds for Reproducibility. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb8829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39752e",
   "metadata": {},
   "source": [
    "## Data Clening & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff9bee",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The following function determines if a given file is an image, checking its extension against common formats like PNG, JPG, and JPEG. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename):\n",
    "    # Check if a file is a PNG or JPG image based on its extension\n",
    "    valid_extensions = ['.png', '.jpg', '.jpeg']\n",
    "    _, ext = os.path.splitext(filename)\n",
    "    return ext.lower() in valid_extensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8324b1b3",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The following function iterates through all files in a specified directory and removes any that do not qualify as image files, as determined by the previously described is_image_file function. This cleanup process ensures the directory contains only PNG or JPG images, streamlining the dataset for image-related tasks.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ca3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_image_files(path):\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        if os.path.isfile(file_path) and not is_image_file(filename):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted {filename} for not being a PNG or JPG image\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9da8682",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The following function scans a specified directory for files exceeding a defined size limit (in megabytes) and deletes any file larger than this threshold. By focusing on file size, it ensures that the directory is free from overly large files, optimizing storage and potentially improving processing efficiency for tasks that involve handling a large number of files.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_large_files(path, max_size_mb):\n",
    "    for filename in os.listdir(path):\n",
    "        file_path = os.path.join(path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)  # Convert to MB\n",
    "            if file_size_mb > max_size_mb:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {filename} for being larger than {max_size_mb}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c68ef",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The following function checks if an image contains transparency by examining its mode and metadata. It supports images with 'RGBA' or 'LA' modes, which include an alpha channel, and also looks for a 'transparency' key in the image's info dictionary. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7cbb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_transparency(img_path):\n",
    "    \"\"\"\n",
    "    Check if an image has transparency.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(img_path) as img:\n",
    "            # Check if the image has an alpha channel\n",
    "            if img.mode in ('RGBA', 'LA') or ('transparency' in img.info):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    except IOError:\n",
    "        print(f\"Error opening {img_path}.\")\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe29e23",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The following function traverses all files in a specified directory and deletes those with transparency, as determined by the previously described has_transparency function. This action ensures the dataset excludes images that contain transparent elements.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e90dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_transparent_images(directory):\n",
    "    \"\"\"\n",
    "    Delete images with transparency in the given directory.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.isfile(file_path) and has_transparency(file_path):\n",
    "            #print(f\"Deleting transparent image: {filename}\")\n",
    "            os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47171b2b",
   "metadata": {},
   "source": [
    "<font size=\"3\"> \n",
    "The following function calculates the total number of files in a specified directory..\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce87206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(path):\n",
    "    return len([name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82c57",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "The following function aims to resize a dataset by ensuring it contains only a specified number of files. It operates by:\n",
    "<br>\n",
    "<ol>\n",
    "<li>Listing all files in the given directory.</li>\n",
    "<li>If the current file count exceeds the target, randomly selecting the surplus files for deletion.</li>\n",
    "<li>Removing the selected files to meet the target dataset size.</li>\n",
    "</ol>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset(path, target_count):\n",
    "    images = [name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))]\n",
    "    if len(images) > target_count:\n",
    "        to_delete = random.sample(images, len(images) - target_count)\n",
    "        for filename in to_delete:\n",
    "            file_path = os.path.join(path, filename)\n",
    "            os.remove(file_path)\n",
    "            #print(f\"Deleted {filename} to reduce the number of images\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c90f0",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "The following function splits an image dataset into training and testing sets for a specified class, according to a given ratio. Here's a brief overview of its steps:\n",
    "<br>\n",
    "<ol>\n",
    "<li>Paths Setup: It organizes the directory structure for the training and testing sets based on the class name.</li>\n",
    "<li>Directory Verification: Checks if the directories for training and testing sets already exist. If not, it creates them.</li>\n",
    "<li>Image Selection: Retrieves all images from the initial class directory, then calculates how many should go into the training set based on the provided ratio.</li>\n",
    "<li>Distribution: Randomly selects images for the training set, with the remainder allocated to the testing set.</li>\n",
    "<li>File Transfer: Moves the selected images to their respective new directories for training and testing.</li>\n",
    "<li>Completion Notification: Prints a summary of how many images were allocated to each set, or indicates if the dataset was previously split.</li>\n",
    "</ol>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef27dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(init_class_path, train_data_path, test_data_path, class_name, train_test_split_ratio):\n",
    "\n",
    "    train_path = os.path.join(train_data_path, class_name)\n",
    "    test_path = os.path.join(test_data_path, class_name)\n",
    "    \n",
    "    if not os.path.exists(test_path):\n",
    "\n",
    "        os.makedirs(train_path, exist_ok=True)\n",
    "        os.makedirs(test_path, exist_ok=True)\n",
    "        \n",
    "        all_images = [f for f in os.listdir(init_class_path) if os.path.isfile(os.path.join(init_class_path, f))]\n",
    "        \n",
    "        num_train = int(len(all_images) * train_test_split_ratio)\n",
    "        \n",
    "        train_images = sample(all_images, num_train)\n",
    "        \n",
    "        test_images = list(set(all_images) - set(train_images))\n",
    "        \n",
    "        for image in train_images:\n",
    "            shutil.move(os.path.join(init_class_path, image), train_path)\n",
    "        \n",
    "        for image in test_images:\n",
    "            shutil.move(os.path.join(init_class_path, image), test_path)\n",
    "        \n",
    "        print(f\"Dataset of class: {class_name} had been splited: {num_train} to train and {len(test_images)} to test.\")\n",
    "    else:\n",
    "        print(\"The dataset is already splited. No action taken.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfd8942",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "The following function checks for and deletes a dataset directory only if both its specified subdirectories (real and fake classes) are empty. It operates as follows:\n",
    "<br>\n",
    "<ol>\n",
    "<li>Empty Check: It first verifies that both the 'real class' and 'fake class' directories contain no files.</li>\n",
    "<li>Deletion: If both directories are confirmed to be empty, the entire dataset directory is deleted.</li>\n",
    "<li>Error Handling: Attempts to delete the dataset directory and reports any errors encountered during the process.</li>\n",
    "<li>Non-empty Case: If either directory contains files, it reports that the dataset is not empty and refrains from deleting anything.</li>\n",
    "</ol>\n",
    "<br>\n",
    "This function ensures that init dataset directories are only removed when they are completely unused, preventing accidental data loss and maintaining data integrity.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_delete(init_real_class_path, init_fake_class_path, inti_dataset_path):\n",
    "    if not os.listdir(init_real_class_path) and not os.listdir(init_fake_class_path):\n",
    "        try:\n",
    "            shutil.rmtree(inti_dataset_path)\n",
    "            print(f\"{inti_dataset_path} has been successfully deleted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {inti_dataset_path}: {e}\")\n",
    "    else:\n",
    "        print(\"Init dataset is not empty. No action taken.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1f4ae",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "The following function streamlines the preparation of an image dataset, combining previously described functions to ensure the data is ready for machine learning tasks. It performs the following key steps:\n",
    "<br>\n",
    "<ol>\n",
    "<li>Preliminary Checks: Verifies the initial dataset directory's existence.</li>\n",
    "<li>Data Cleaning: Utilizes delete_non_image_files, delete_large_files, and delete_transparent_images to remove unsuitable images based on format, size, and transparency.</li>\n",
    "<li>Dataset Balancing: Adjusts the number of images in each class to ensure a balanced dataset, employing the create_subset function.</li>\n",
    "<li>Data Splitting: Splits the dataset into training and testing sets with a specified ratio using train_test_split.</li>\n",
    "<li>Cleanup: Checks and deletes empty class directories post-processing through check_and_delete.</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data_preprocessing(inti_dataset_path, init_real_class_path, init_fake_class_path, max_image_size_mb, train_data_path, test_data_path, train_test_split_ratio):\n",
    "\n",
    "    if os.path.exists(inti_dataset_path):\n",
    "        \n",
    "        # Step 1: Delete non-image files\n",
    "        delete_non_image_files(init_real_class_path)\n",
    "        delete_non_image_files(init_fake_class_path)\n",
    "        \n",
    "        # Step 2: Delete images larger than 5MB\n",
    "        delete_large_files(init_real_class_path, max_image_size_mb)\n",
    "        delete_large_files(init_fake_class_path, max_image_size_mb)\n",
    "\n",
    "        # Step 3: Delete transparent images\n",
    "        delete_transparent_images(init_real_class_path)\n",
    "        delete_transparent_images(init_fake_class_path)\n",
    "        \n",
    "        # Step 4: Count the images remaining in path1\n",
    "        c1 = count_images(init_real_class_path)\n",
    "\n",
    "        # Step 5: Adjust the number of images in path2 to be 2*c1\n",
    "        c2 = c1\n",
    "        create_subset(init_fake_class_path, c2)\n",
    "        \n",
    "        # Step 6: Train - Test split\n",
    "        train_test_split(init_real_class_path, train_data_path, test_data_path, 'REAL', train_test_split_ratio)\n",
    "        train_test_split(init_fake_class_path, train_data_path, test_data_path, 'FAKE', train_test_split_ratio)\n",
    "        \n",
    "        # Step 7: Delete init folders\n",
    "        check_and_delete(init_real_class_path, init_fake_class_path, inti_dataset_path)\n",
    "    else:\n",
    "        print(\"Init dataset does not exist. Please downloaded from: https://www.kaggle.com/datasets/superpotato9/dalle-recognition-dataset/data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c559d8f",
   "metadata": {},
   "source": [
    "## Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split_ratio = 0.7\n",
    "max_image_size_mb = 5\n",
    "\n",
    "set_seeds()\n",
    "init_data_preprocessing(inti_dataset_path, init_real_class_path, init_fake_class_path, max_image_size_mb, train_data_path, test_data_path, train_test_split_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad04992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full_ml",
   "language": "python",
   "name": "full_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
